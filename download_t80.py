#!/usr/bin/env python3
"""
Download and process Lc0 T80 training data.

The T80 dataset contains positions from Leela Chess Zero's Training Run 1,
generated by a ~3200 ELO engine. This is the gold standard for AlphaZero-style
supervised learning.

Key advantages over Lichess evaluated positions:
- Soft policy targets (MCTS visit distribution, not just best move)
- Positions from strong self-play (consistent quality)
- Native binary format (fast loading, no parsing)
- Higher quality training signal

Data source: https://storage.lczero.org/files/training_data/test80/
Format: V6 (8356 bytes per position)

Usage:
    # Download to a volume first (cheap CPU instance)
    python download_t80.py --output ./data/t80 --num-files 50

    # Then train on expensive GPU
    python train.py --data ./data/t80
"""

import os
import sys
import gzip
import struct
import tarfile
import tempfile
import argparse
import requests
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Tuple, Optional, Dict
import multiprocessing as mp
from io import BytesIO
import re
from datetime import datetime
import threading
import time

# Lc0 V6 format constants
V6_VERSION = 6
NUM_POLICY_MOVES = 1858  # Lc0's policy size
MIN_POLICY_MOVES = 1800
MAX_POLICY_MOVES = 2000

# Lc0 move encoding - maps (from_sq, to_sq, promo) to Lc0 policy index
# This is computed at module load time
LC0_MOVE_TO_IDX = {}
LC0_IDX_TO_MOVE = {}

def _init_lc0_move_tables():
    """Initialize Lc0's move encoding tables (1858 moves)"""
    global LC0_MOVE_TO_IDX, LC0_IDX_TO_MOVE

    idx = 0

    # Queen moves (including rook/bishop): 56 per square (8 directions * 7 distances)
    # But only for valid destinations
    for from_sq in range(64):
        from_rank = from_sq // 8
        from_file = from_sq % 8

        # 8 directions: N, NE, E, SE, S, SW, W, NW
        directions = [
            (1, 0), (1, 1), (0, 1), (-1, 1),
            (-1, 0), (-1, -1), (0, -1), (1, -1)
        ]

        for dir_idx, (dr, df) in enumerate(directions):
            for dist in range(1, 8):
                to_rank = from_rank + dr * dist
                to_file = from_file + df * dist
                if 0 <= to_rank < 8 and 0 <= to_file < 8:
                    to_sq = to_rank * 8 + to_file
                    LC0_MOVE_TO_IDX[(from_sq, to_sq, None)] = idx
                    LC0_IDX_TO_MOVE[idx] = (from_sq, to_sq, None)
                    idx += 1

    # Knight moves: 8 per square (but only valid ones)
    knight_deltas = [
        (2, 1), (2, -1), (-2, 1), (-2, -1),
        (1, 2), (1, -2), (-1, 2), (-1, -2)
    ]
    for from_sq in range(64):
        from_rank = from_sq // 8
        from_file = from_sq % 8
        for dr, df in knight_deltas:
            to_rank = from_rank + dr
            to_file = from_file + df
            if 0 <= to_rank < 8 and 0 <= to_file < 8:
                to_sq = to_rank * 8 + to_file
                key = (from_sq, to_sq, None)
                if key not in LC0_MOVE_TO_IDX:
                    LC0_MOVE_TO_IDX[key] = idx
                    LC0_IDX_TO_MOVE[idx] = key
                    idx += 1

    # Underpromotions (knight, bishop, rook - queen is default)
    # Pawns promote from rank 6 (white) or rank 1 (black), viewed as rank 6 after flip
    KNIGHT, BISHOP, ROOK = 2, 3, 4
    for promo in [KNIGHT, BISHOP, ROOK]:
        for from_file in range(8):
            from_sq = 6 * 8 + from_file  # Rank 6 (0-indexed)
            # Straight promotion
            to_sq = 7 * 8 + from_file
            LC0_MOVE_TO_IDX[(from_sq, to_sq, promo)] = idx
            LC0_IDX_TO_MOVE[idx] = (from_sq, to_sq, promo)
            idx += 1
            # Capture left
            if from_file > 0:
                to_sq = 7 * 8 + from_file - 1
                LC0_MOVE_TO_IDX[(from_sq, to_sq, promo)] = idx
                LC0_IDX_TO_MOVE[idx] = (from_sq, to_sq, promo)
                idx += 1
            # Capture right
            if from_file < 7:
                to_sq = 7 * 8 + from_file + 1
                LC0_MOVE_TO_IDX[(from_sq, to_sq, promo)] = idx
                LC0_IDX_TO_MOVE[idx] = (from_sq, to_sq, promo)
                idx += 1

    print(f"Initialized Lc0 move tables: {idx} moves")

_init_lc0_move_tables()


def _format_bytes(num_bytes: float) -> str:
    if num_bytes < 1024:
        return f"{num_bytes:.0f}B"
    if num_bytes < 1024 ** 2:
        return f"{num_bytes / 1024:.1f}KB"
    if num_bytes < 1024 ** 3:
        return f"{num_bytes / (1024 ** 2):.1f}MB"
    return f"{num_bytes / (1024 ** 3):.2f}GB"


def _format_duration(seconds: float) -> str:
    if seconds <= 0:
        return "--:--"
    minutes, sec = divmod(int(seconds), 60)
    hours, minutes = divmod(minutes, 60)
    if hours > 0:
        return f"{hours:d}:{minutes:02d}:{sec:02d}"
    return f"{minutes:02d}:{sec:02d}"


def _get_remote_size(url: str) -> Optional[int]:
    try:
        response = requests.head(url, timeout=30, allow_redirects=True)
        response.raise_for_status()
        size = response.headers.get("content-length")
        return int(size) if size else None
    except Exception:
        return None


def _validate_tar_file(path: str) -> Tuple[bool, Optional[str]]:
    try:
        with tarfile.open(path, "r") as tar:
            tar.getmembers()
        return True, None
    except Exception as e:
        return False, str(e)


class DownloadProgress:
    def __init__(self, total_files: int):
        self.total_files = total_files
        self.files_done = 0
        self.files_failed = 0
        self.files_cached = 0
        self.bytes_downloaded = 0
        self.bytes_total = 0
        self.active: Dict[str, Dict[str, int]] = {}
        self.start_time = time.time()
        self.lock = threading.Lock()

    def mark_cached(self, filename: str, size: int) -> None:
        with self.lock:
            self.files_done += 1
            self.files_cached += 1
            if size > 0:
                self.bytes_total += size
                self.bytes_downloaded += size

    def register(self, filename: str, total_size: int) -> None:
        with self.lock:
            self.active[filename] = {"downloaded": 0, "total": total_size}
            if total_size > 0:
                self.bytes_total += total_size

    def update(self, filename: str, delta: int) -> None:
        with self.lock:
            entry = self.active.get(filename)
            if entry is None:
                return
            entry["downloaded"] += delta
            self.bytes_downloaded += delta

    def complete(self, filename: str, success: bool) -> None:
        with self.lock:
            if filename in self.active:
                self.active.pop(filename, None)
            self.files_done += 1
            if not success:
                self.files_failed += 1

    def snapshot(self) -> dict:
        with self.lock:
            return {
                "total_files": self.total_files,
                "files_done": self.files_done,
                "files_failed": self.files_failed,
                "files_cached": self.files_cached,
                "bytes_downloaded": self.bytes_downloaded,
                "bytes_total": self.bytes_total,
                "active": dict(self.active),
                "start_time": self.start_time,
            }


def _download_progress_reporter(progress: DownloadProgress, stop_event: threading.Event,
                                interval_s: float = 5.0) -> None:
    while not stop_event.wait(interval_s):
        snapshot = progress.snapshot()
        if snapshot["files_done"] >= snapshot["total_files"] and not snapshot["active"]:
            return

        elapsed = time.time() - snapshot["start_time"]
        rate = snapshot["bytes_downloaded"] / elapsed if elapsed > 0 else 0.0
        if snapshot["bytes_total"] > 0:
            pct = (snapshot["bytes_downloaded"] / snapshot["bytes_total"]) * 100
            eta = (snapshot["bytes_total"] - snapshot["bytes_downloaded"]) / rate if rate > 0 else 0.0
            size_status = f"{_format_bytes(snapshot['bytes_downloaded'])}/{_format_bytes(snapshot['bytes_total'])} ({pct:.1f}%)"
        else:
            eta = 0.0
            size_status = f"{_format_bytes(snapshot['bytes_downloaded'])}"

        active_items = []
        active_list = sorted(snapshot["active"].items(), key=lambda x: x[1]["downloaded"], reverse=True)
        if len(active_list) > 8:
            active_list = active_list[:8]
        for name, stats in active_list:
            total = stats["total"]
            if total > 0:
                active_items.append(f"{name} {stats['downloaded'] * 100 / total:.0f}%")
            else:
                active_items.append(f"{name} {_format_bytes(stats['downloaded'])}")

        active_status = f" | active: {', '.join(active_items)}" if active_items else ""
        tqdm.write(
            f"Download status: {snapshot['files_done']}/{snapshot['total_files']} files, "
            f"{size_status}, {_format_bytes(rate)}/s, ETA {_format_duration(eta)}{active_status}"
        )


def _v6_record_size(policy_size: int) -> int:
    return policy_size * 4 + 832 + 92


def _detect_v6_layout(data: bytes) -> Tuple[int, int, int, int]:
    scan_limit = min(len(data), 4 * 1024 * 1024)
    scan = data[:scan_limit]
    for version in (V6_VERSION, 7):
        pattern = struct.pack('<I', version)
        indices = []
        start = 0
        while True:
            idx = scan.find(pattern, start)
            if idx == -1:
                break
            indices.append(idx)
            start = idx + 1
        if len(indices) < 2:
            continue
        diffs = [b - a for a, b in zip(indices, indices[1:]) if b - a > 1000]
        if not diffs:
            continue
        diff_counts: Dict[int, int] = {}
        for d in diffs:
            diff_counts[d] = diff_counts.get(d, 0) + 1
        record_size = max(diff_counts.items(), key=lambda kv: kv[1])[0]
        if record_size <= 0:
            continue
        mod_counts: Dict[int, int] = {}
        for idx in indices:
            mod = idx % record_size
            mod_counts[mod] = mod_counts.get(mod, 0) + 1
        offset = max(mod_counts.items(), key=lambda kv: kv[1])[0]
        if record_size <= 924 or (record_size - 924) % 4 != 0:
            continue
        policy_size = (record_size - 924) // 4
        if not (MIN_POLICY_MOVES <= policy_size <= MAX_POLICY_MOVES):
            continue
        if offset + 8 <= scan_limit:
            input_format, = struct.unpack_from('<I', scan, offset + 4)
            if input_format > 10:
                continue
        return version, policy_size, record_size, offset
    return V6_VERSION, NUM_POLICY_MOVES, _v6_record_size(NUM_POLICY_MOVES), 0


def parse_v6_record(data: bytes, policy_size: int, expected_version: int) -> Optional[dict]:
    """
    Parse a single V6 training record (8356 bytes).

    Returns dict with:
        - planes: 104 x uint64 bitboards
        - probabilities: 1858 floats (MCTS visit distribution)
        - result_q: game outcome [-1, 1]
        - result_d: draw probability
        - best_q: position evaluation
        - castling: (us_ooo, us_oo, them_ooo, them_oo)
        - side_to_move: 0 or 1
        - rule50: fifty-move counter
    """
    if len(data) != _v6_record_size(policy_size):
        return None

    offset = 0

    # Version and input_format (8 bytes)
    version, input_format = struct.unpack_from('<II', data, offset)
    offset += 8

    if version != expected_version:
        return None

    # Probabilities: 1858 floats (7432 bytes)
    probabilities = np.frombuffer(data, dtype=np.float32, count=policy_size, offset=offset)
    offset += policy_size * 4

    # Planes: 104 uint64 (832 bytes) - raw bitboards
    planes = np.frombuffer(data, dtype=np.uint64, count=104, offset=offset)
    offset += 104 * 8

    # Castling and other flags (8 bytes)
    castling_us_ooo = data[offset]
    castling_us_oo = data[offset + 1]
    castling_them_ooo = data[offset + 2]
    castling_them_oo = data[offset + 3]
    side_to_move_or_ep = data[offset + 4]
    rule50 = data[offset + 5]
    invariance_info = data[offset + 6]
    dummy = data[offset + 7]
    offset += 8

    # Q-values and other floats (44 bytes)
    floats = struct.unpack_from('<11f', data, offset)
    root_q, best_q, root_d, best_d, root_m, best_m, plies_left = floats[:7]
    result_q, result_d, played_q, played_d = floats[7:11]
    offset += 44

    # More floats (12 bytes)
    played_m, orig_q, orig_d = struct.unpack_from('<3f', data, offset)
    offset += 12

    # orig_m, visits, played_idx, best_idx, policy_kld, reserved (20 bytes)
    orig_m, = struct.unpack_from('<f', data, offset)
    offset += 4
    visits, = struct.unpack_from('<I', data, offset)
    offset += 4
    played_idx, best_idx = struct.unpack_from('<HH', data, offset)
    offset += 4
    policy_kld, = struct.unpack_from('<f', data, offset)
    offset += 4
    reserved, = struct.unpack_from('<I', data, offset)

    return {
        'probabilities': probabilities.copy(),
        'planes': planes.copy(),
        'result_q': result_q,
        'result_d': result_d,
        'best_q': best_q,
        'castling': (castling_us_ooo, castling_us_oo, castling_them_ooo, castling_them_oo),
        'side_to_move': side_to_move_or_ep & 1,
        'rule50': rule50,
        'best_idx': best_idx,
        'played_idx': played_idx,
        'visits': visits,
    }


def lc0_planes_to_board(planes: np.ndarray, castling: tuple,
                         side_to_move: int, rule50: int) -> np.ndarray:
    """
    Convert Lc0's 104x64-bit planes to our 18x8x8 format.

    Lc0 plane layout (first 13 planes are current position):
        0: Our pawns
        1: Our knights
        2: Our bishops
        3: Our rooks
        4: Our queens
        5: Our kings
        6: Their pawns
        7: Their knights
        8: Their bishops
        9: Their rooks
        10: Their queens
        11: Their kings
        12: Repetition count (we ignore)

    IMPORTANT: Lc0 planes are ALWAYS from the side-to-move's perspective.
    Our encoding is always from White's perspective.
    So when black is to move, we need to flip the board vertically and
    swap "our" pieces (0-5) with "their" pieces (6-11).

    Remaining planes are history (we ignore for now).
    """
    board = np.zeros((18, 8, 8), dtype=np.float32)
    planes_arr = np.asarray(planes)
    if planes_arr.dtype.kind not in ("u", "i"):
        try:
            planes_arr = planes_arr.astype(np.uint64)
        except (TypeError, ValueError) as exc:
            raise ValueError(f"Invalid planes dtype: {planes_arr.dtype}") from exc
    else:
        planes_arr = planes_arr.astype(np.uint64, copy=False)

    # When black to move, Lc0's "our" is black and "their" is white
    # We need to swap and flip vertically
    is_black_to_move = (side_to_move == 1)

    # Extract piece bitboards (first 12 planes)
    for lc0_plane_idx in range(12):
        bitboard = int(planes_arr[lc0_plane_idx])
        if bitboard == 0:
            continue

        # Determine which plane this maps to in our encoding
        if is_black_to_move:
            # Swap: Lc0's "our" (0-5) -> our black (6-11)
            #       Lc0's "their" (6-11) -> our white (0-5)
            if lc0_plane_idx < 6:
                our_plane_idx = lc0_plane_idx + 6  # Black pieces
            else:
                our_plane_idx = lc0_plane_idx - 6  # White pieces
        else:
            our_plane_idx = lc0_plane_idx

        for sq in range(64):
            if bitboard & (1 << sq):
                rank = sq // 8
                file = sq % 8
                # Flip vertically when black to move
                if is_black_to_move:
                    rank = 7 - rank
                board[our_plane_idx, rank, file] = 1.0

    # Side to move (plane 12) - always set for white to move
    if not is_black_to_move:
        board[12, :, :] = 1.0

    # Castling rights (planes 13-16)
    # Lc0 castling is from side-to-move perspective, we need white's perspective
    us_ooo, us_oo, them_ooo, them_oo = castling
    if is_black_to_move:
        # Swap: "us" is black, "them" is white
        if them_oo:  # White kingside
            board[13, :, :] = 1.0
        if them_ooo:  # White queenside
            board[14, :, :] = 1.0
        if us_oo:  # Black kingside
            board[15, :, :] = 1.0
        if us_ooo:  # Black queenside
            board[16, :, :] = 1.0
    else:
        if us_oo:  # White kingside
            board[13, :, :] = 1.0
        if us_ooo:  # White queenside
            board[14, :, :] = 1.0
        if them_oo:  # Black kingside
            board[15, :, :] = 1.0
        if them_ooo:  # Black queenside
            board[16, :, :] = 1.0

    # En passant - extracted from planes if available
    # For simplicity, we'll skip en passant in initial version
    # (plane 17 stays zeros)

    return board


def process_chunk_file(chunk_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, dict]:
    """
    Process a single .chunk.gz file and extract training data.

    Returns:
        boards: (N, 18, 8, 8) float32
        policies: (N, 1858) float32 - soft targets!
        values: (N,) float32
        stats: dict with record counts and layout info
    """
    stats = {
        "version": None,
        "policy_size": None,
        "record_size": None,
        "offset": None,
        "total_records": 0,
        "skipped_record": 0,
        "skipped_board": 0,
        "skipped_policy": 0,
        "nan_value": 0,
        "kept": 0,
    }
    boards_list = []
    policies_list = []
    values_list = []

    with gzip.open(chunk_path, 'rb') as f:
        data = f.read()

    version, policy_size, record_size, offset = _detect_v6_layout(data)
    num_records = (len(data) - offset) // record_size

    if num_records <= 0:
        return None, None, None, stats

    stats.update({
        "version": version,
        "policy_size": policy_size,
        "record_size": record_size,
        "offset": offset,
        "total_records": num_records,
    })

    for i in range(num_records):
        start = offset + i * record_size
        record_data = data[start:start + record_size]
        record = parse_v6_record(record_data, policy_size, version)

        if record is None:
            stats["skipped_record"] += 1
            continue

        # Convert to our format
        try:
            board = lc0_planes_to_board(
                record['planes'],
                record['castling'],
                record['side_to_move'],
                record['rule50']
            )
        except (TypeError, ValueError):
            stats["skipped_board"] += 1
            continue

        # Use soft policy targets (the key advantage of T80!)
        policy = record['probabilities']

        # Normalize policy to sum to 1
        with np.errstate(invalid="ignore", over="ignore"):
            policy_sum = float(np.sum(policy))
        if not np.isfinite(policy_sum) or policy_sum <= 0:
            stats["skipped_policy"] += 1
            continue  # Skip positions with no policy
        policy = policy / policy_sum

        # Value from position evaluation (best_q) or game result
        # best_q is the MCTS-backed evaluation, result_q is game outcome
        # We use best_q for position evaluation (more stable than game result)
        # NOTE: best_q is from side-to-move's perspective, but our encoding
        # is always from White's perspective, so flip for black
        value = record['best_q']
        if record['side_to_move'] == 1:  # Black to move
            value = -value

        if not np.isfinite(value):
            stats["nan_value"] += 1
            continue

        # Clamp to [-1, 1]
        value = max(-1.0, min(1.0, value))

        boards_list.append(board)
        policies_list.append(policy)
        values_list.append(value)
        stats["kept"] += 1

    if not boards_list:
        return None, None, None, stats

    return (
        np.array(boards_list, dtype=np.float32),
        np.array(policies_list, dtype=np.float32),
        np.array(values_list, dtype=np.float32),
        stats
    )


def download_file(url: str, output_path: str,
                  chunk_size: int = 8192,
                  show_progress: bool = True) -> bool:
    """Download a file with progress bar"""
    try:
        response = requests.get(url, stream=True, timeout=30)
        response.raise_for_status()

        total_size = int(response.headers.get('content-length', 0))

        with open(output_path, 'wb') as f:
            if show_progress and total_size:
                pbar = tqdm(total=total_size, unit='B', unit_scale=True,
                           desc=os.path.basename(output_path))

            for chunk in response.iter_content(chunk_size=chunk_size):
                f.write(chunk)
                if show_progress and total_size:
                    pbar.update(len(chunk))

            if show_progress and total_size:
                pbar.close()

        return True
    except Exception as e:
        print(f"Error downloading {url}: {e}")
        return False


def list_t80_files(base_url: str = "https://storage.lczero.org/files/training_data/test80/") -> List[str]:
    """List available T80 training files"""
    try:
        response = requests.get(base_url, timeout=30)
        response.raise_for_status()

        # Parse HTML to find .tar files
        files = re.findall(r'training-run1-test80-\d{8}-\d{4}\.tar', response.text)
        return sorted(set(files))
    except Exception as e:
        print(f"Error listing files: {e}")
        return []


def download_single_file(args):
    """Download a single file - for parallel downloading"""
    url, tar_path, file_idx, total_files, progress, verify = args
    filename = os.path.basename(tar_path)
    expected_size = _get_remote_size(url) if verify else None

    if os.path.exists(tar_path):
        size = os.path.getsize(tar_path)
        if expected_size and size < expected_size:
            try:
                os.unlink(tar_path)
            except Exception:
                pass
        else:
            if verify and not expected_size:
                valid, error = _validate_tar_file(tar_path)
                if not valid:
                    try:
                        os.unlink(tar_path)
                    except Exception:
                        pass
                    return tar_path, False, f"[{file_idx+1}/{total_files}] {filename} invalid cache: {error}"
            progress.mark_cached(filename, size)
            return tar_path, True, f"[{file_idx+1}/{total_files}] {filename} (cached, {_format_bytes(size)})"

    try:
        response = requests.get(url, stream=True, timeout=60)
        response.raise_for_status()

        total_size = int(response.headers.get('content-length', 0))
        if expected_size is None and total_size > 0:
            expected_size = total_size
        progress.register(filename, total_size)

        with open(tar_path, 'wb') as f:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=65536):
                if not chunk:
                    continue
                f.write(chunk)
                downloaded += len(chunk)
                progress.update(filename, len(chunk))

        if expected_size and downloaded < expected_size:
            progress.complete(filename, False)
            try:
                os.unlink(tar_path)
            except Exception:
                pass
            return tar_path, False, f"[{file_idx+1}/{total_files}] {filename} incomplete download"

        if verify:
            valid, error = _validate_tar_file(tar_path)
            if not valid:
                progress.complete(filename, False)
                try:
                    os.unlink(tar_path)
                except Exception:
                    pass
                return tar_path, False, f"[{file_idx+1}/{total_files}] {filename} invalid tar: {error}"

        progress.complete(filename, True)
        size_label = _format_bytes(total_size) if total_size > 0 else _format_bytes(downloaded)
        return tar_path, True, f"[{file_idx+1}/{total_files}] {filename} ({size_label})"
    except Exception as e:
        progress.complete(filename, False)
        return tar_path, False, f"[{file_idx+1}/{total_files}] {filename} FAILED: {e}"


def download_and_process_t80(
    output_dir: str = "./data/t80",
    num_files: int = 10,
    num_positions: int = None,
    start_date: str = None,
    workers: int = 4,
    keep_tar: bool = False,
):
    """
    Download and process T80 training data.

    Args:
        output_dir: Where to save processed .npz files
        num_files: Number of .tar files to download
        num_positions: Stop after this many positions (None = no limit)
        start_date: Start from this date (YYYYMMDD format)
        workers: Number of parallel workers for processing AND downloading
        keep_tar: Keep downloaded .tar files
    """
    os.makedirs(output_dir, exist_ok=True)

    base_url = "https://storage.lczero.org/files/training_data/test80/"

    print(f"\n{'='*60}")
    print("DOWNLOADING LC0 T80 TRAINING DATA")
    print(f"{'='*60}")
    print(f"Source: {base_url}")
    print(f"Output: {output_dir}")
    print(f"Files to download: {num_files}")
    print(f"Workers: {workers} (parallel downloads + parallel processing)")
    print(f"{'='*60}\n")

    # Get list of available files
    print("Fetching file list...")
    files = list_t80_files(base_url)

    if not files:
        print("ERROR: Could not fetch file list from storage.lczero.org")
        return

    print(f"Found {len(files)} T80 training files")

    # Filter by date if specified
    if start_date:
        files = [f for f in files if start_date in f or f > f"training-run1-test80-{start_date}"]

    # Take requested number
    files = files[:num_files]

    print(f"Will download {len(files)} files")

    total_positions = 0
    chunk_idx = 0
    all_boards = []
    all_policies = []
    all_values = []

    # PHASE 1: Download all files in parallel (much faster!)
    print(f"\n--- PHASE 1: Parallel Download ({min(workers, len(files))} concurrent) ---")
    progress = DownloadProgress(len(files))
    stop_event = threading.Event()
    reporter_thread = threading.Thread(
        target=_download_progress_reporter,
        args=(progress, stop_event),
        daemon=True,
    )
    reporter_thread.start()

    verify_downloads = True
    tar_urls = {}
    download_args = []
    for idx, filename in enumerate(files):
        url = base_url + filename
        tar_path = os.path.join(output_dir, filename)
        tar_urls[tar_path] = url
        download_args.append((url, tar_path, idx, len(files), progress, verify_downloads))

    downloaded_files = []
    max_workers = min(workers, len(files))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(download_single_file, args) for args in download_args]
        for future in tqdm(as_completed(futures), total=len(futures), desc="Downloading"):
            tar_path, success, msg = future.result()
            tqdm.write(f"  {msg}")
            if success:
                downloaded_files.append(tar_path)

    stop_event.set()
    reporter_thread.join(timeout=2)

    snapshot = progress.snapshot()
    print(
        f"\nDownloaded {len(downloaded_files)}/{len(files)} files "
        f"(cached: {snapshot['files_cached']}, failed: {snapshot['files_failed']})"
    )

    # PHASE 2: Process downloaded files
    print(f"\n--- PHASE 2: Process Files ({workers} workers) ---")
    for file_idx, tar_path in enumerate(downloaded_files):
        filename = os.path.basename(tar_path)
        print(f"\n[{file_idx + 1}/{len(downloaded_files)}] Processing {filename}...")

        try:
            attempt = 0
            tar = None
            while attempt < 2:
                try:
                    tar = tarfile.open(tar_path, 'r')
                    _ = tar.getmembers()
                    break
                except Exception as e:
                    if tar is not None:
                        try:
                            tar.close()
                        except Exception:
                            pass
                    if attempt == 0:
                        print(f"  Warning: {filename} invalid ({e}), re-downloading...")
                        try:
                            os.unlink(tar_path)
                        except Exception:
                            pass
                        url = tar_urls.get(tar_path)
                        if not url:
                            raise
                        if not download_file(url, tar_path, show_progress=True):
                            raise
                        valid, error = _validate_tar_file(tar_path)
                        if not valid:
                            raise RuntimeError(f"re-download invalid: {error}")
                    else:
                        raise
                attempt += 1

            if tar is None:
                raise RuntimeError("failed to open tar file")

            with tar:
                members = [m for m in tar.getmembers() if m.name.endswith('.gz')]

                # Extract all chunks to temp files for parallel processing
                temp_files = []
                for member in members:
                    chunk_data = tar.extractfile(member).read()
                    tmp = tempfile.NamedTemporaryFile(suffix='.gz', delete=False)
                    tmp.write(chunk_data)
                    tmp.close()
                    temp_files.append(tmp.name)

                # Process chunks in parallel using ProcessPoolExecutor
                try:
                    with ProcessPoolExecutor(max_workers=workers) as executor:
                        results = list(tqdm(
                            executor.map(process_chunk_file, temp_files),
                            total=len(temp_files),
                            desc="Processing chunks"
                        ))

                    tar_total_records = 0
                    tar_kept_records = 0
                    tar_skipped_record = 0
                    tar_skipped_board = 0
                    tar_skipped_policy = 0
                    tar_nan_value = 0
                    tar_versions = set()
                    tar_policy_sizes = set()
                    tar_record_sizes = set()
                    tar_offsets = set()

                    for boards, policies, values, stats in results:
                        if stats:
                            tar_total_records += stats.get("total_records", 0)
                            tar_kept_records += stats.get("kept", 0)
                            tar_skipped_record += stats.get("skipped_record", 0)
                            tar_skipped_board += stats.get("skipped_board", 0)
                            tar_skipped_policy += stats.get("skipped_policy", 0)
                            tar_nan_value += stats.get("nan_value", 0)
                            if stats.get("version") is not None:
                                tar_versions.add(stats.get("version"))
                            if stats.get("policy_size") is not None:
                                tar_policy_sizes.add(stats.get("policy_size"))
                            if stats.get("record_size") is not None:
                                tar_record_sizes.add(stats.get("record_size"))
                            if stats.get("offset") is not None:
                                tar_offsets.add(stats.get("offset"))

                        if boards is not None:
                            all_boards.append(boards)
                            all_policies.append(policies)
                            all_values.append(values)
                            total_positions += len(boards)

                        # Check position limit
                        if num_positions and total_positions >= num_positions:
                            break
                    layout_version = ", ".join(str(v) for v in sorted(tar_versions)) or "unknown"
                    layout_policy = ", ".join(str(v) for v in sorted(tar_policy_sizes)) or "unknown"
                    layout_record = ", ".join(str(v) for v in sorted(tar_record_sizes)) or "unknown"
                    layout_offset = ", ".join(str(v) for v in sorted(tar_offsets)) or "unknown"
                    print(
                        f"  Records kept: {tar_kept_records:,}/{tar_total_records:,} "
                        f"(bad_record {tar_skipped_record:,}, bad_board {tar_skipped_board:,}, "
                        f"bad_policy {tar_skipped_policy:,}, bad_value {tar_nan_value:,})"
                    )
                    print(
                        f"  Layout: v{layout_version}, policy={layout_policy}, "
                        f"record={layout_record}, offset={layout_offset}"
                    )
                finally:
                    # Clean up temp files
                    for tmp_path in temp_files:
                        try:
                            os.unlink(tmp_path)
                        except:
                            pass

                # Save chunk when we have enough
                current_size = sum(len(b) for b in all_boards)
                if current_size >= 1_000_000:  # Save every 1M positions
                    chunk_name = f"chunk_{chunk_idx:04d}.npz"
                    chunk_path = os.path.join(output_dir, chunk_name)

                    np.savez(
                        chunk_path,
                        boards=np.concatenate(all_boards),
                        policies=np.concatenate(all_policies),
                        values=np.concatenate(all_values),
                    )

                    print(f"\n  Saved {chunk_path} ({current_size:,} positions)")

                    chunk_idx += 1
                    all_boards = []
                    all_policies = []
                    all_values = []

            # Remove tar file if not keeping
            if not keep_tar and os.path.exists(tar_path):
                os.unlink(tar_path)

        except Exception as e:
            print(f"Error processing {filename}: {e}")
            continue

        # Check position limit
        if num_positions and total_positions >= num_positions:
            print(f"\nReached position limit: {num_positions:,}")
            break

    # Save remaining data
    if all_boards:
        chunk_name = f"chunk_{chunk_idx:04d}.npz"
        chunk_path = os.path.join(output_dir, chunk_name)

        np.savez(
            chunk_path,
            boards=np.concatenate(all_boards),
            policies=np.concatenate(all_policies),
            values=np.concatenate(all_values),
        )

        print(f"\n  Saved {chunk_path} ({sum(len(b) for b in all_boards):,} positions)")
        chunk_idx += 1

    print(f"\n{'='*60}")
    print("DOWNLOAD COMPLETE")
    print(f"{'='*60}")
    print(f"Total positions: {total_positions:,}")
    print(f"Chunks saved: {chunk_idx}")
    print(f"Output directory: {output_dir}")
    print(f"\nTo train:")
    print(f"  python train.py --data {output_dir} --policy-size 1858")


def verify_t80_dataset(data_dir: str):
    """Verify the T80 dataset"""
    import glob

    files = sorted(glob.glob(os.path.join(data_dir, "chunk_*.npz")))

    if not files:
        print(f"No .npz files found in {data_dir}")
        return

    print(f"\nVerifying T80 dataset in {data_dir}")
    print(f"Found {len(files)} chunk files")

    data = np.load(files[0])
    boards = data['boards']
    policies = data['policies']
    values = data['values']

    print(f"\nFirst chunk stats:")
    print(f"  Boards shape: {boards.shape}")
    print(f"  Policies shape: {policies.shape}")
    print(f"  Values shape: {values.shape}")

    print(f"\nValue distribution:")
    print(f"  Min: {values.min():.3f}")
    print(f"  Max: {values.max():.3f}")
    print(f"  Mean: {values.mean():.3f}")
    print(f"  Std: {values.std():.3f}")

    print(f"\nPolicy stats (soft targets):")
    print(f"  Mean entropy: {-(policies * np.log(policies + 1e-10)).sum(axis=1).mean():.3f}")
    print(f"  Mean max prob: {policies.max(axis=1).mean():.3f}")

    # Count value distribution
    winning = np.sum(values > 0.5)
    equal = np.sum((values >= -0.5) & (values <= 0.5))
    losing = np.sum(values < -0.5)

    print(f"\nValue breakdown:")
    print(f"  Winning (>0.5): {winning} ({100*winning/len(values):.1f}%)")
    print(f"  Equal (-0.5 to 0.5): {equal} ({100*equal/len(values):.1f}%)")
    print(f"  Losing (<-0.5): {losing} ({100*losing/len(values):.1f}%)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Download and process Lc0 T80 training data",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Download 10 files (~10-20M positions)
    python download_t80.py --output ./data/t80 --num-files 10

    # Download specific number of positions
    python download_t80.py --output ./data/t80 --positions 50000000

    # Verify downloaded data
    python download_t80.py --output ./data/t80 --verify
"""
    )

    parser.add_argument("--output", type=str, default="./data/t80",
                        help="Output directory")
    parser.add_argument("--num-files", type=int, default=10,
                        help="Number of .tar files to download")
    parser.add_argument("--positions", type=int, default=None,
                        help="Stop after this many positions")
    parser.add_argument("--start-date", type=str, default=None,
                        help="Start from date (YYYYMMDD)")
    parser.add_argument("--workers", type=int, default=4,
                        help="Number of parallel workers")
    parser.add_argument("--keep-tar", action="store_true",
                        help="Keep downloaded .tar files")
    parser.add_argument("--verify", action="store_true",
                        help="Verify existing dataset")

    args = parser.parse_args()

    if args.verify:
        verify_t80_dataset(args.output)
    else:
        download_and_process_t80(
            output_dir=args.output,
            num_files=args.num_files,
            num_positions=args.positions,
            start_date=args.start_date,
            workers=args.workers,
            keep_tar=args.keep_tar,
        )
